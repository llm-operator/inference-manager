{{- if .Values.autoscaling.enableKeda }}
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: {{ include "inference-manager-engine.fullname" . }}
  labels:
    {{- include "inference-manager-engine.labels" . | nindent 4 }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "inference-manager-engine.fullname" . }}
  pollingInterval: 15
  cooldownPeriod: 30
  minReplicaCount:  {{ .Values.autoscaling.scaledobject.minReplicaCount }}
  maxReplicaCount:  {{ .Values.autoscaling.scaledobject.maxReplicaCount }}
  # uncomment this if it is desired to scale to zero.
  # idleReplicaCount: 0
  triggers:
    - type: prometheus
      metadata:
        serverAddress: {{ .Values.autoscaling.scaledobject.prometheusServerAddr }}
        metricName: gpu-util
        metricType: Value
        query: |-
          sum(avg_over_time(DCGM_FI_DEV_GPU_UTIL{pod=~"llm-operator-inference-manager-engine.*"}[1m]))
        threshold: '50'
   - type: prometheus
      metadata:
        serverAddress: {{ .Values.autoscaling.scaledobject.prometheusServerAddr }}
        metricName: active-requests
        metricType: Value
        query: |-
          sum(avg_over_time(llm_operator_inference_manager_serever_completion_num_active_requests{model_id=~".*"}[1m]))
        threshold: '1'
{{- end }}
