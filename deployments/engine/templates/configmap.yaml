apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "inference-manager-engine.fullname" . }}
  labels:
    {{- include "inference-manager-engine.labels" . | nindent 4 }}
data:
  config.yaml: |
    ollama:
      keepAlive: {{ .Values.ollama.keepAlive }}
      numParallel: {{ .Values.maxConcurrentRequests }}
      forceSpreading: {{ .Values.ollama.forceSpreading }}
      debug: {{ .Values.ollama.debug }}
    vllm:
      model: {{ .Values.vllm.model }}
      numGpus: {{ .Values.vllm.numGpus }}
    llmEngine: {{ .Values.llmEngine }}
    llmPort: {{ .Values.llmPort }}
    healthPort: {{ .Values.healthPort }}
    {{- with .Values.preloadedModelIds }}
    preloadedModelIds:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    {{- with .Values.modelContextLengths }}
    modelContextLengths:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    inferenceManagerServerWorkerServiceAddr: {{ .Values.inferenceManagerServerWorkerServiceAddr }}
    modelManagerServerWorkerServiceAddr: {{ .Values.global.worker.controlPlaneAddr | default .Values.modelManagerServerWorkerServiceAddr }}
    worker:
      tls:
        enable: {{ .Values.global.worker.tls.enable }}
    objectStore:
      s3:
        endpointUrl: {{ .Values.global.objectStore.s3.endpointUrl }}
        region: {{ .Values.global.objectStore.s3.region }}
        bucket: {{ .Values.global.objectStore.s3.bucket }}
