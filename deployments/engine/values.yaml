global:
  objectStore:
    s3:
      endpointUrl:
      region:
      bucket:

  awsSecret:
    name:
    accessKeyIdKey:
    secretAccessKeyKey:

  worker:
    registrationKeySecret:
      name:
      key:
    tls:
      enable: false
    # If specified, use this as the address for accessing the control-plane services.
    controlPlaneAddr: ""

  controlPlaneAddr: ""

ollama:
  keepAlive: 10m

vllm:
  numGpus: 1

llmEngine: ollama
llmPort: 8080

preloadedModelIds:

# The following default values work if model-manager-server runs in the same namespace.
inferenceManagerServerWorkerServiceAddr: inference-manager-server-worker-service-grpc:8082
modelManagerServerWorkerServiceAddr: model-manager-server-worker-service-grpc:8082

replicaCount: 1
image:
  repository: public.ecr.aws/cloudnatix/llm-operator/inference-manager-engine
  pullPolicy: IfNotPresent

serviceAccount:
  create: false
  name: ""

podAnnotations:
nodeSelector:
affinity:
tolerations:

version:

resources:
  gpu: 1

autoscaling:
  # TODO(kenji): Enable once Keda installation is supported in the Terraform provisioning.
  enableKeda: false
  scaledobject:
    minReplicaCount: 1
    maxReplicaCount: 2
    prometheusServerAddr:

resources:
  requests:
    cpu: "250m"
    memory: "500Mi"
  limits:
    cpu: "250m"

podSecurityContext:
  fsGroup: 2000
securityContext:
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL
  runAsNonRoot: true
  runAsUser: 1000
