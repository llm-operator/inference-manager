global:
  objectStore:
    s3:
      endpointUrl:
      region:
      bucket:

  awsSecret:
    name:
    accessKeyIdKey:
    secretAccessKeyKey:

  worker:
    registrationKeySecret:
      name:
      key:
    tls:
      enable: false
    # If specified, use this as the address for accessing the control-plane services.
    controlPlaneAddr: ""

  controlPlaneAddr: ""

# Change this to "alpha" to run the alpha command.
commandName: run

runtime:
  name: ollama
  runtimeImages:
    ollama: mirror.gcr.io/ollama/ollama:0.3.6
    vllm: mirror.gcr.io/vllm/vllm-openai:v0.5.5
  imagePullPolicy: IfNotPresent
  modelResources:
    # sample-model:
    #   requests:
    #     cpu: "100m"
    #     memory: "128Mi"
    #   limits:
    #     cpu: "200m"
    #     memory: "256Mi"
    #   volume:
    #     storageClassName: "standard"
    #     size: "1Gi"
    #     accessMode: "ReadWriteOnce"
  defaultResources:
    requests:
      cpu: "1000m"
      memory: "500Mi"
    limits:
      nvidia.com/gpu: 1
    # volume:
    #   storageClassName: "fast"
    #   size: "100Gi"
    #   accessMode: "ReadWriteOnce"

ollama:
  # Keep models in memory for a long period of time as we don't want end users to
  # hit slowness due to GPU memory loading.
  keepAlive: 96h
  # If set, Ollama attemts to all GPUs in a node.
  # Setting this to true was mentioned in https://github.com/ollama/ollama/issues/5494
  # to support multiple H100s, but this setting didn't help.
  forceSpreading: false
  debug: false
  runnersDir: /tmp/ollama-runners

vllm:
  numGpus: 1

# maxConcurentRequests is the maximun number of requests procssed in parallel.
maxConcurrentRequests: 0

llmEngine: ollama
llmPort: 8080
healthPort: 8081

preloadedModelIds:

modelContextLengths:

# The following default values work if model-manager-server runs in the same namespace.
inferenceManagerServerWorkerServiceAddr: inference-manager-server-worker-service-grpc:8082
modelManagerServerWorkerServiceAddr: model-manager-server-worker-service-grpc:8082

replicaCount: 1
image:
  repository: public.ecr.aws/cloudnatix/llm-operator/inference-manager-engine
  pullPolicy: IfNotPresent

serviceAccount:
  # Set this to false if an existing service account is used.
  create: true
  name: ""

podAnnotations:
nodeSelector:
affinity:
tolerations:

version:

autoscaling:
  # TODO(kenji): Enable once Keda installation is supported in the Terraform provisioning.
  enableKeda: false
  scaledobject:
    minReplicaCount: 1
    maxReplicaCount: 2
    prometheusServerAddr:

resources:
  requests:
    cpu: "1000m"
    memory: "500Mi"
  # Do not specify CPU/memory limits as it varies on model and SLO requirements.
  # Also most cases a GPU node is dedicated to the engine.
  limits:
    nvidia.com/gpu: 1

podSecurityContext:
  fsGroup: 2000
securityContext:
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL
  runAsNonRoot: true
  runAsUser: 1000

persistentVolume:
  # If true, engine will create/use a PVC. If false, use emptyDir
  enabled: false
  # If defined, the engine uses the given PVC and does not create a new one.
  # NOTE: PVC must be manually created before the volume is bound.
  existingClaim:

  storageClassName:
  # If defined, the engine uses the existing PV that has been provisioned in advance.
  volumeName:

  size: 100Gi
  accessModes:
  - ReadWriteOnce

  volumeBindingMode:
  selector:

readinessProbe:
  httpGet:
    path: /ready
    port: health
    scheme: HTTP
  initialDelaySeconds: 3
  failureThreshold: 5
  timeoutSeconds: 3
