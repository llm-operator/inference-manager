syntax = "proto3";

package llmoperator.inference.server.v1;

import "api/v1/inference_server.proto";
import "api/v1/inference_server_embeddings.proto";

option go_package = "github.com/llm-operator/inference-manager/api/v1";

message EngineStatus {
  message SyncStatus {
    // in_progress_model_ids is a list of model ids that are currently being synced.
    repeated string in_progress_model_ids = 1;
  }
  string engine_id = 1;
  repeated string model_ids = 2;
  SyncStatus sync_status = 3;
}

message HeaderValue {
  repeated string values = 1;
}

message HttpResponse {
  int32 status_code = 1;
  string status = 2;
  map<string, HeaderValue> header = 3;
  // body is empty for server sent events.
  bytes body = 4;
}

message ServerSentEvent {
  bytes data = 1;
  bool is_last_event = 2;
}

message TaskResult {
  string task_id = 1;
  oneof message {
    HttpResponse http_response = 2;
    ServerSentEvent server_sent_event = 3;
  }
}

message ProcessTasksRequest {
  oneof message {
    EngineStatus engine_status = 1;
    TaskResult task_result = 2;
  }
}

message TaskRequest {
  oneof request {
    llmoperator.chat.server.v1.CreateChatCompletionRequest chat_completion = 1;
    llmoperator.embeddings.server.v1.CreateEmbeddingRequest embedding = 2;
  }
}

message Task {
  string id = 1;

  TaskRequest request = 4;
  map<string, HeaderValue> header = 3;

  // For backward compatibility for old engine.
  llmoperator.chat.server.v1.CreateChatCompletionRequest deprecated_chat_completion_request = 2;

  // Next ID: 5
}

message ProcessTasksResponse {
  Task new_task = 1;
}

service InferenceWorkerService {
  rpc ProcessTasks(stream ProcessTasksRequest) returns (stream ProcessTasksResponse) {}

}
