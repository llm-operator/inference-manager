syntax = "proto3";

package llmoperator.inference.server.v1;

import "api/v1/inference_server.proto";

option go_package = "github.com/llm-operator/inference-manager/api/v1";

message EngineStatus {
  string engine_id = 1;
  repeated string model_ids = 2;
}

message HeaderValue {
  repeated string values = 1;
}

message HttpResponse {
  int32 status_code = 1;
  string status = 2;
  map<string, HeaderValue> header = 3;
  // body is empty for server sent events.
  bytes body = 4;
}

message ServerSentEvent {
  bytes data = 1;
  bool is_last_event = 2;
}

message TaskResult {
  string task_id = 1;
  oneof message {
    HttpResponse http_response = 2;
    ServerSentEvent server_sent_event = 3;
  }
}

message ProcessTasksRequest {
  oneof message {
    EngineStatus engine_status = 1;
    TaskResult task_result = 2;
  }
}

message Task {
  string id = 1;
  llmoperator.chat.server.v1.CreateChatCompletionRequest request = 2;
  map<string, HeaderValue> header = 3;
}

message ProcessTasksResponse {
  Task new_task = 1;
}

service InferenceWorkerService {
  rpc ProcessTasks(stream ProcessTasksRequest) returns (stream ProcessTasksResponse) {}

}
