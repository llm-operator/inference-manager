llmPort: 8080
ollama:
  keepAlive: 10m
objectStore:
inferenceManagerServerWorkerServiceAddr: server:8082
modelManagerServerWorkerServiceAddr:

llmEngine: ollama

debug:
  standalone: true
